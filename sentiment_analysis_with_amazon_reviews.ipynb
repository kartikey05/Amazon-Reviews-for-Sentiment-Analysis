{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3caP98aAtiD_"
      },
      "source": [
        "# Amazon Reviews Dataset\n",
        "\n",
        "This dataset contains several million reviews of Amazon products, with the reviews separated into two classes for positive and negative reviews. The two classes are evenly balanced here.\n",
        "\n",
        "This is a large dataset, and the version that I am using here only has the text as a feature with no other metadata. This makes this an interesting dataset for doing NLP work. It is data written by users, so it's like that there are various typos, nonstandard spellings, and other variations that you may not find in curated sets of published text.\n",
        "\n",
        "In this notebook, I will do some very simple text processing and then try out two fairly unoptimized deep learning models:\n",
        "1. A convolutional neural net\n",
        "2. A recurrent neural net\n",
        "These models should achieve results that are within a couple percent of state of the art at predicting the binary sentiment of the reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZWl2QO6tiEM",
        "outputId": "9bb5733a-2c69-4f84-8411-7268ec3f2e61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'test.ft.txt', 'train.ft.txt', 'sample_data']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import bz2\n",
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "import re\n",
        "\n",
        "%matplotlib inline\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "#After you've uploaded your dataset, you can check the current directory by running:\n",
        "# Print the current directory contents\n",
        "import os\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjqPhzMAtiEW"
      },
      "source": [
        "## Reading the text\n",
        "\n",
        "The text is held in a compressed format. Luckily, we can still read it line by line. The first word gives the label, so we have to convert that into a number and then take the rest to be the comment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "VxtmIFEPtiEX"
      },
      "outputs": [],
      "source": [
        "# Define a function to extract labels and texts from a file\n",
        "def get_labels_and_texts(file):\n",
        "    # Initialize empty lists to store labels and texts\n",
        "    labels = []\n",
        "    texts = []\n",
        "\n",
        "    # Open the specified file for reading using 'utf-8' encoding\n",
        "    with open(file, 'r', encoding='utf-8') as f:\n",
        "        # Iterate through each line in the file\n",
        "        for line in f:\n",
        "            # Extract the label from the line (assuming a specific format in the file)\n",
        "            # Convert the label to an integer, subtract 1, and append to the labels list\n",
        "            labels.append(int(line[9]) - 1)\n",
        "\n",
        "            # Extract the text from the line, starting from the 10th character to the end\n",
        "            # Remove leading and trailing whitespaces and append to the texts list\n",
        "            texts.append(line[10:].strip())\n",
        "\n",
        "    # Convert the labels list to a NumPy array and return the tuple of labels and texts\n",
        "    return np.array(labels), texts\n",
        "\n",
        "# Call the function to get labels and texts for the training data\n",
        "train_labels, train_texts = get_labels_and_texts('train.ft.txt')\n",
        "\n",
        "# Call the function to get labels and texts for the test data\n",
        "test_labels, test_texts = get_labels_and_texts('test.ft.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6mBQWMVtiEY"
      },
      "source": [
        "## Text Preprocessing\n",
        "\n",
        "The first thing I'm going to do to process the text is to lowercase everything and then remove non-word characters. I replace these with spaces since most are going to be punctuation. Then I'm going to just remove any other characters (like letters with accents). It could be better to replace some of these with regular ascii characters but I'm just going to ignore that here. It also turns out if you look at the counts of the different characters that there are very few unusual characters in this corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "rytJQgTntiEZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "NON_ALPHANUM = re.compile(r'[\\W]')\n",
        "NON_ASCII = re.compile(r'[^a-z0-1\\s]')\n",
        "\n",
        "def normalize_texts(texts):\n",
        "    normalized_texts = []\n",
        "    for text in texts:\n",
        "        lower = text.lower()\n",
        "        no_punctuation = NON_ALPHANUM.sub(r' ', lower)\n",
        "        no_non_ascii = NON_ASCII.sub(r'', no_punctuation)\n",
        "        normalized_texts.append(no_non_ascii)\n",
        "    return normalized_texts\n",
        "\n",
        "# Ensure train_texts and test_texts are lists\n",
        "train_texts = normalize_texts(train_texts)\n",
        "test_texts = normalize_texts(test_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y34RFYGtiEa"
      },
      "source": [
        "## Train/Validation Split\n",
        "Now I'm going to set aside 20% of the training set for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "3z4TiuFntiEb"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, random_state=57643892, test_size=0.2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aPUhGxVtiEc"
      },
      "source": [
        "Keras provides some tools for converting text to formats that are useful in deep learning models. I've already done some processing, so now I will just run a Tokenizer using the top 12000 words as features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "OHw4UfIHtiEd"
      },
      "outputs": [],
      "source": [
        "MAX_FEATURES = 12000\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_FEATURES)\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "train_texts = tokenizer.texts_to_sequences(train_texts)\n",
        "val_texts = tokenizer.texts_to_sequences(val_texts)\n",
        "test_texts = tokenizer.texts_to_sequences(test_texts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xke_Xr7DtiEe"
      },
      "source": [
        "## Padding Sequences\n",
        "In order to use batches effectively, I'm going to need to take my sequences and turn them into sequences of the same length. I'm just going to make everything here the length of the longest sentence in the training set. I'm not dealing with this here, but it may be advantageous to have variable lengths so that each batch contains sentences of similar lengths. This might help mitigate issues that arise from having too many padded elements in a sequence. There are also different padding modes that might be useful for different models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "EPU8lSRdtiEf"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = max(len(train_ex) for train_ex in train_texts)\n",
        "\n",
        "train_texts = tf.keras.preprocessing.sequence.pad_sequences(train_texts, maxlen=MAX_LENGTH)\n",
        "val_texts = tf.keras.preprocessing.sequence.pad_sequences(val_texts, maxlen=MAX_LENGTH)\n",
        "test_texts = tf.keras.preprocessing.sequence.pad_sequences(test_texts, maxlen=MAX_LENGTH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqUcNScFtiEg"
      },
      "source": [
        "## Convolutional Neural Net Model\n",
        "\n",
        "I'm just using fairly simple models here. This CNN has an embedding with a dimension of 64, 3 convolutional layers with the first two having match normalization and max pooling and the last with global max pooling. The results are then passed to a dense layer and then the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Ez3K8VqLtiEh"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "    sequences = tf.keras.layers.Input(shape=(MAX_LENGTH,))\n",
        "    embedded = tf.keras.layers.Embedding(MAX_FEATURES, 64)(sequences)\n",
        "    x = tf.keras.layers.Conv1D(64, 3, activation='relu')(embedded)\n",
        "\n",
        "    # BatchNormalization is now part of tf.keras.layers\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    x = tf.keras.layers.MaxPool1D(3)(x)\n",
        "    x = tf.keras.layers.Conv1D(64, 5, activation='relu')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.MaxPool1D(5)(x)\n",
        "    x = tf.keras.layers.Conv1D(64, 5, activation='relu')(x)\n",
        "    x = tf.keras.layers.GlobalMaxPool1D()(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(100, activation='relu')(x)\n",
        "    predictions = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    model = tf.keras.models.Model(inputs=sequences, outputs=predictions)\n",
        "    model.compile(\n",
        "        optimizer='rmsprop',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['binary_accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = build_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09eQQQvKtiEi",
        "outputId": "75122af7-ab68-4556-d82d-be73ae58bf3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2, Validation Loss: 0.6930, Validation Accuracy: 0.5065\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on validation data at the end of each epoch\n",
        "val_loss, val_acc = model.evaluate(val_texts, val_labels, batch_size=batch_size, verbose=0)\n",
        "\n",
        "print(f'Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmR8NO1otiEj"
      },
      "source": [
        "Once this finishes training, we should find that we get an accuracy of around 94% for this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "UoVz_7EatiEj",
        "outputId": "c76981f1-573a-493c-e2c4-9a3d2cfcb06b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3869/3869 [==============================] - 48s 12ms/step\n",
            "Accuracy score: 0.5065\n",
            "F1 score: 0.6644\n",
            "ROC AUC score: 0.5228\n"
          ]
        }
      ],
      "source": [
        "preds = model.predict(test_texts)\n",
        "print('Accuracy score: {:0.4}'.format(accuracy_score(test_labels, 1 * (preds > 0.5))))\n",
        "print('F1 score: {:0.4}'.format(f1_score(test_labels, 1 * (preds > 0.5))))\n",
        "print('ROC AUC score: {:0.4}'.format(roc_auc_score(test_labels, preds)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKlUF5TntiEl"
      },
      "source": [
        "## Recurrent Neural Net Model\n",
        "For an RNN model I'm also going to use a simple model. This has an embedding, two GRU layers, followed by 2 dense layers and then the output layer. I'm using the CuDNNGRU rather than GRU because the former will run much faster (over a factor of 10 I think on Kaggle's servers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "XNnbuwW6tiEm"
      },
      "outputs": [],
      "source": [
        "def build_rnn_model():\n",
        "    sequences = layers.Input(shape=(MAX_LENGTH,))\n",
        "    embedded = layers.Embedding(MAX_FEATURES, 64)(sequences)\n",
        "    x = layers.GRU(128, return_sequences=True)(embedded)\n",
        "    x = layers.GRU(128)(x)\n",
        "    x = layers.Dense(32, activation='relu')(x)\n",
        "    x = layers.Dense(100, activation='relu')(x)\n",
        "    predictions = layers.Dense(1, activation='sigmoid')(x)\n",
        "    model = models.Model(inputs=sequences, outputs=predictions)\n",
        "    model.compile(\n",
        "        optimizer='rmsprop',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['binary_accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "rnn_model = build_rnn_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bi35C5kHtiEm",
        "outputId": "ced58b5d-e78a-4233-baed-c2a1085fc60b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "442/753 [================>.............] - ETA: 7:17 - loss: 0.5179 - binary_accuracy: 0.7197"
          ]
        }
      ],
      "source": [
        "# Assuming train_texts, val_texts, and test_texts are already tokenized and padded sequences\n",
        "rnn_model.fit(\n",
        "    np.array(train_texts),\n",
        "    np.array(train_labels),\n",
        "    batch_size=128,\n",
        "    epochs=1,\n",
        "    validation_data=(np.array(val_texts), np.array(val_labels))\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzfdEwNctiEn"
      },
      "source": [
        "And we should find that this model will end up with an accuracy similar to the CNN model. I haven't bothered to set the seeds, but it can go as high as 95%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haEQ-GEVtiEo",
        "outputId": "3d7fe65b-12c6-4376-cd4e-b5ae73c24e52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy score: 0.9502\n",
            "F1 score: 0.9504\n",
            "ROC AUC score: 0.9881\n"
          ]
        }
      ],
      "source": [
        "preds = rnn_model.predict(test_texts)\n",
        "print('Accuracy score: {:0.4}'.format(accuracy_score(test_labels, 1 * (preds > 0.5))))\n",
        "print('F1 score: {:0.4}'.format(f1_score(test_labels, 1 * (preds > 0.5))))\n",
        "print('ROC AUC score: {:0.4}'.format(roc_auc_score(test_labels, preds)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s3X22YBtiEo"
      },
      "source": [
        "## What else could we do?\n",
        "\n",
        "There are lots of things I haven't tried here. I think the original data from Amazon has other fields that could be added to the model. Additionally, we haven't added any global features from the samples such as length, character level features, and more. We could even attempt to run character-level deep learning models, which might be able to reduce sensitivity to misspellings. In online reviews, character level features could be quite important as users could intentionally misspell things to avoid moderation. However, these models are already performing at well over 90% so at this point any gains are going to be pretty small."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}